{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1: Predicting Car Price ##\n",
    "This data is a regression problem, trying to predict car price.\n",
    "\n",
    "The followings describe the features.\n",
    "\n",
    "name : brandname of the car\n",
    "year : released year car selling_price : price for buying the car km_driven : kilometer that the car has been driven\n",
    "fuel : fuel that the car can used\n",
    "seller_type : someone that customer buy the car\n",
    "transmission : manual or anutomatic\n",
    "owner : the number of owners that how many people used it.\n",
    "mileage : a term use to express the fuel efficiency of a vehicle.\n",
    "engine : size of engine\n",
    "max_power : maximum power output that car can make.\n",
    "torque : a measurement of your car's ability to do work.\n",
    "seats : the capacity that one car can carry people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some important library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Cars.csv') #Load the data from .csv file so 'data/Cars.csv' is path to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #There are 8128 samples, 12 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.EDA (Explotory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the most important step to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = df, x = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = df, x = 'km_driven')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = df, x = 'fuel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = df, x = 'transmission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = df, x = 'owner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = df, x = 'engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = df, x = 'max_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = df, x = 'seats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change 'name' -> 'brand'\n",
    "df.rename(columns = {'name' : 'brand'\n",
    "                     }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map First Owner to 1 .... Test Drive Car to 5\n",
    "label_mapping = {\n",
    "    'First Owner' : 1,\n",
    "    'Second Owner' : 2,\n",
    "    'Third Owner' : 3,\n",
    "    'Fourth & Above Owner' : 4,\n",
    "    'Test Drive Car' : 5\n",
    "}\n",
    "categorical_data = df['owner']\n",
    "df['owner'] = [label_mapping[label] for label in categorical_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all rows that having 'CNG' and 'LPG'\n",
    "df = df[~df['fuel'].isin(['CNG', 'LPG'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 'mileage' remove kmpl and convert to float\n",
    "df['mileage'] = df['mileage'].str.split().str[0].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 'engine' remove 'CC' and covert to float\n",
    "df['engine'] = df['engine'].str.split().str[0].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 'max_power' remove ' bhp' and convert to float\n",
    "df['max_power'] = df['max_power'].str.split().str[0].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 'brand' remove all model of cars, we will get only the name.\n",
    "df['brand'] = df['brand'].str.split().str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminated column 'torque'\n",
    "df =df.drop(columns=['torque'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminated 'owner' that having Test-Drive Car\n",
    "df = df[df['owner'] != 5]\n",
    "#Let's checks...\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before plot correlation matrix we will encoded something..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encoded for 'fuel'\n",
    "fuel_le = LabelEncoder()\n",
    "df['fuel'] = fuel_le.fit_transform(df['fuel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoded for 'transmission'\n",
    "trans_le = LabelEncoder()\n",
    "df['transmission'] = trans_le.fit_transform(df['transmission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to see relation between variable that having more correlated or less correlated \n",
    "plt.figure(figsize = (15,10)) #size of table\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm') #use heatmap that having cool-warm colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note That : Features must not be corelated and we should not pick both of them.You can choose for only one that has the most importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to see between features (X) and label (y) \n",
    "import ppscore as pps\n",
    "\n",
    "dfcopy = df.copy()\n",
    "\n",
    "matrix_df = pps.matrix(dfcopy)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore') #plotting..\n",
    "\n",
    "plt.figure(figsize = (15,8)) #size of table\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap='Blues', linewidths=0.5, annot=True) #use heatmap that having blues colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the important feature that related to label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['max_power', 'mileage', 'km_driven']] #Choose some features and store to X variable\n",
    "\n",
    "y = np.log(df['selling_price']) #Choose label and store to y variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test set is test accuracy for training set.\n",
    "- Test set should never be touch when you compare the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #import scikit-learn library\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "#spilt X and y to X_train, X_test, y_train, y_test that have size of test set = 0.2 and \n",
    "#seed = 42 (random factor for which one go to training set or test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to find mean and median to fill the missing values of the data (Imputation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the distribution plot, mean and median of max_power\n",
    "sns.displot(data=df, x='max_power')\n",
    "df['max_power'].mean(),df['max_power'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the distribution plot, mean and median of mileage\n",
    "sns.displot(data=df, x='mileage')\n",
    "df['mileage'].mean(),df['mileage'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the distribution plot, mean and median of km_driven\n",
    "sns.displot(data=df, x='km_driven')\n",
    "df['km_driven'].mean(),df['km_driven'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the distribution plot, mean and median of selling_price\n",
    "sns.displot(y_train)\n",
    "df['selling_price'].mean(),df['selling_price'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['max_power'].fillna(X_train['max_power'].median(), inplace=True) #mean is good representation\n",
    "X_train['mileage'].fillna(X_train['mileage'].median(), inplace=True) #mean is good representation\n",
    "X_train['km_driven'].fillna(X_train['km_driven'].median(), inplace=True) #mean is good representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace missing value by adding X_train to X_test because your model learn the patterns in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing value by adding X_train to X_test because your model learn the patterns in the training set\n",
    "X_test['max_power'].fillna(X_train['max_power'].median(), inplace=True) #mean is good representation\n",
    "X_test['mileage'].fillna(X_train['mileage'].median(), inplace=True) #mean is good representation\n",
    "X_test['km_driven'].fillna(X_train['km_driven'].median(), inplace=True) #mean is good representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.fillna(y_train.mean(), inplace=True)\n",
    "y_test.fillna(y_train.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of columns.\n",
    "col_dict = {'max_power':1, 'mileage':2, 'km_driven':3}\n",
    "\n",
    "# Detect outliers in each variable using box plots.\n",
    "plt.figure(figsize=(20,30))\n",
    "\n",
    "for variable,i in col_dict.items():\n",
    "                     plt.subplot(5,4,i)\n",
    "                     plt.boxplot(X_train[variable])\n",
    "                     plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_count(col, data = X_train): #create function that calculated the outliers\n",
    "    \n",
    "    # calculate your 25% quatile and 75% quatile\n",
    "    q75, q25 = np.percentile(data[col], [75, 25])\n",
    "    \n",
    "    # calculate your inter quatile\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # min_val and max_val\n",
    "    min_val = q25 - (iqr*1.5)\n",
    "    max_val = q75 + (iqr*1.5)\n",
    "    \n",
    "    # count number of outliers, which are the data that are less than min_val or more than max_val calculated above\n",
    "    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
    "    \n",
    "    # calculate the percentage of the outliers\n",
    "    outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
    "    \n",
    "    if(outlier_count > 0):\n",
    "        print(\"\\n\"+15*'-' + col + 15*'-'+\"\\n\")\n",
    "        print('Number of outliers: {}'.format(outlier_count))\n",
    "        print('Percent of data that is outlier: {}%'.format(outlier_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns: #Indicate the outlier\n",
    "    outlier_count(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- will help your model understand and everything in a unique unit scale.        \n",
    "- to solved  the plobem between data that have the more value and the less value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# select features that we have choosen.\n",
    "features = ['max_power', 'mileage', 'km_driven']\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "X_train[features] = scaler.fit_transform(X_train[features])\n",
    "X_test[features]  = scaler.transform(X_test[features])\n",
    "\n",
    "#x = (x - mean) / std\n",
    "#why do we want to scale our data before data analysis / machine learning\n",
    "\n",
    "#allows your machine learning model to catch the pattern/relationship faster\n",
    "#faster convergence\n",
    "\n",
    "#how many ways to scale\n",
    "#standardardization <====current way : The mean is good representation\n",
    "# (x - mean) / std\n",
    "#--> when your data follows normal distribution\n",
    "\n",
    "#normalization <---another way : The mean is bad representation\n",
    "# (x - x_min) / (x_max - x_min)\n",
    "#---> when your data DOES NOT follow normal distribution (e.g., audio, signal, image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check shapes of all X_train, X_test, y_train, y_test\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross Validation + Grid search       \n",
    "        _Cross Validation : find which one is the best algorithm.      \n",
    "        Grid search : find the best version of algorithm's name._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Libraries for model evaluation\n",
    "\n",
    "# Models that we will be using, put them in a list\n",
    "algorithms = [LinearRegression(), SVR(), KNeighborsRegressor(), DecisionTreeRegressor(random_state = 42), \n",
    "              RandomForestRegressor(n_estimators = 100, random_state = 42)]\n",
    "\n",
    "# The names of the models\n",
    "algorithm_names = [\"Linear Regression\", \"SVR\", \"KNeighbors Regressor\", \"Decision-Tree Regressor\", \"Random-Forest Regressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "#lists for keeping mse\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "#defining splits\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i, model in enumerate(algorithms):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error') #find mse, score and mean of mse\n",
    "    print(f\"{algorithm_names[i]} - Score: {scores}; Mean: {scores.mean()}\") #Higher is better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'bootstrap': [True], 'max_depth': [5, 10, None],\n",
    "              'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 1)\n",
    "\n",
    "grid = GridSearchCV(estimator = rf, \n",
    "                    param_grid = param_grid, \n",
    "                    cv = kfold, \n",
    "                    n_jobs = -1, \n",
    "                    return_train_score=True, \n",
    "                    refit=True,\n",
    "                    scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit your grid_search\n",
    "grid.fit(X_train, y_train);  #fit means start looping all the possible parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find your grid_search's best score\n",
    "best_mse = grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse  # ignore the minus because it's neg_mean_squared_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that : mse : [0, infty) | 0 is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  #we are using regression models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "yhat = grid.predict(X_test)\n",
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhat) #check yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original = np.exp(yhat) #Turn yhat to the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_original) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting which feature is important for prediction can help us interpret the results. There are several ways: algorithm, permutation, and shap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stored in this variable\n",
    "#note that grid here is random forest\n",
    "rf = grid.best_estimator_\n",
    "\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot\n",
    "plt.barh(X.columns, rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm...let's sort first\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.barh(X.columns[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random-Forest Regressor Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation way :         \n",
    "This method will randomly shuffle each feature and compute the change in the model’s performance. The features which impact the performance the most are the most important one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "\n",
    "#let's plot\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(X.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Random-Forest Regressor Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shap way :        \n",
    "The SHAP interpretation can be used (it is model-agnostic) to compute the feature importances. It is using the Shapley values from game theory to estimate the how does each feature contribute to the prediction. It can be easily installed (pip install shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap provides plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'model/car_price_prediction.model'\n",
    "pickle.dump(grid, open(filename, 'wb'))\n",
    "\n",
    "# save the scaler to disk\n",
    "scaler_path = 'scale/scaler.prep'\n",
    "pickle.dump(scaler, open(scaler_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "scaler = pickle.load(open('scale/scaler.prep', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.DataFrame([[74,23.4,145500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = scaler.transform(sample_df)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicted Price of the car: {np.exp(loaded_model.predict(sample_df))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is about car price prediction.There are 8128 samples,12 features and 1 label.It is the regression problem because the label, selling price, is continuous. There are a lots of datas in .CSV file. In the CSV file, there are name year selling_price km_driven fuel seller_type transmission mileage engine max_power torque seats.To deal with this datas we need to manage and understand the nature of the datas. For predicting prices of cars we not only trusted the correlation matrix or PPS but also understand the reality of features that have affect to selling prices. If we already understand the reality of the data we can choose the datas that have related to selling prices. In these notebook will choose the most 3 important features that have affected such as max_power, mileage, km_driven. Why we choose all this features? because when plotting the graph as we can see that we will discover what features are good or not good. The good will have related with the label,selling prices and the bad one is not related to selling prices not only for plotting but also looking to correlation matrix so that all the feature that I have choosen. when we already choosen the features, We need to decide the label that is selling prices. The next step, We will check the missing values of the data, To filling the missing value(imputation), We will fill with mean or median because we don't want the high MSE(Mean Squere Error). After that we will check the outliers and scales the datas. The scaling have 2 methods that is standadization and normalization. We will use standadization for scaling data when the data follow normal distribution and for the normalization when the data don't follow normalization so this is the method calls preprocessing. The next steps is Modeling. In this steps we will find the precise and accurate algorithm. There are the algoritms names : \"Linear Regression\", \"SVR\", \"KNeighbors Regressor\", \"Decision-Tree Regressor\", \"Random-Forest Regressor\" we will explain each of the algorithm next paragraph so the step that find algoritms calls Cross-validation and to do the best version of algorithm we calls Grid search after that we will testing to predict selling prices from the data by training and testing set from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap provides plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclution, The most important features are 3 things but there can be more. some feature can be eliminated when you look to the correlation matrix. The matrix can tell us that we should not use feature that are highly correlated. In this case you can see that between max_power and engine were not use for features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, The algorithm that perform well is Random-Forest Regression. because the mean of MSE is the highest from all algoritms. For picking the best algorithm, We will look to the mean of MSE of each algoritm. If mean of MSE in each algorithm is higher, the better to choose algorithm is that so we will explain what is each of algoritms that we have.\n",
    "         \n",
    "Linear Regression -> is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.  \n",
    "\n",
    "SVR -> is a machine learning algorithm that extends the principles of Support Vector Machines (SVMs) to the task of regression. While SVMs are primarily used for classification tasks, SVR is designed for predicting continuous numeric values. SVR aims to find a regression function that fits the data while controlling the margin of error.  \n",
    "\n",
    "KNeighbors Regressor -> is a supervised machine learning algorithm used for regression tasks. It is a non-parametric and instance-based algorithm that makes predictions based on the similarity of input data points to their neighboring data points. KNeighborsRegressor is an example of a lazy learner, meaning it doesn't build an explicit model during training, but instead, it stores the training data and uses it directly for making predictions.        \n",
    "\n",
    "Decision-Tree Regressor -> is a supervised machine learning algorithm used for regression tasks. It's a type of decision tree algorithm that predicts a continuous target variable based on the features of the input data. Decision trees are versatile and easy to understand, making them a popular choice for both classification and regression problems.      \n",
    "      \n",
    "Random-Forest Regressor -> is a machine learning algorithm that belongs to the ensemble learning family. It's designed for regression tasks and is an extension of the concept of decision trees. The key idea behind a Random Forest Regressor is to build multiple decision trees and combine their predictions to improve overall accuracy and reduce overfitting.         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
